{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "15bff421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "827b3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(query: str) -> list:\n",
    "    response = requests.get(f'https://www.gp.se/nyheter/västsverige?q={query}')\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    # get the results count\n",
    "    num_results = soup.find('p', class_=re.compile('c-search-results__title')).text\n",
    "    num_results = [int(s) for s in num_results.split() if s.isdigit()][1]\n",
    "    results_per_page = 10\n",
    "    links = []\n",
    "    dates = []\n",
    "    for page in range(1, 2):#int(num_results/results_per_page) + 1):\n",
    "        print (\"Current page: \" + str(page))\n",
    "        url = f\"https://www.gp.se/nyheter/sverige?q={query}&page={page}\"\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        links += [\"https://gp.se\"+a.attrs.get('href') for\n",
    "                  a in soup.find_all('a', class_=re.compile('c-teaser__link'))]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "52d3cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(source_page):\n",
    "    \n",
    "    PATTERN_HEADLINE_TEXT = re.compile(r\"(?<=\\\"headline\\\">)[A-Öa-ö 0-9.!?,-s–]+(?=<\\/h1>)\")\n",
    "    PATTERN_BROAD_TEXT = re.compile(r\"(?<=fomaker.se\\/idf\\/1.0\\\">)[A-Öa-ö -0-9.!?,–]+(?=<\\/element>)\")\n",
    "    PATTERN_LINK_TEXT = re.compile(r\"(?<=\\\"c-article__body__content\\\">)[\\nA-Öa-ö -0-9.!?,–<>=:0-9]+\")\n",
    "    ARTICLE_DATE = re.compile(r'(?<=datetime\\=)[A-Öa-ö 0-9.!?,-:\"]+(?=\\sitemprop\\=\\\"datePublished\\\">)')\n",
    "    \n",
    "    title = re.findall(PATTERN_HEADLINE_TEXT, source_page)\n",
    "    date = datetime.fromisoformat(re.findall(ARTICLE_DATE, source_page)[0][1:-2])\n",
    "    collected = []\n",
    "    \n",
    "    for match in re.finditer(PATTERN_LINK_TEXT, source_page):\n",
    "        text = match.group() + \" \"\n",
    "        span = match.span()\n",
    "        data = (span, text)\n",
    "        collected.append(data)\n",
    "\n",
    "    if not collected:\n",
    "        return \"NULL\",\"NULL\",\"NULL\"\n",
    "    if not title:\n",
    "        title = [\"NULL\"]\n",
    "    if not date:\n",
    "        date = [\"NULL\"]\n",
    "\n",
    "    # Find span for broad text\n",
    "    first_broad_span = collected[0][0][0]\n",
    "    last_broad_span = collected[-1][0][1]\n",
    "\n",
    "    # correct span for broad texts now when broadtext chunk span is found\n",
    "    collected = [((n[0][0] - first_broad_span, n[0][1] - first_broad_span), n[1]) for n in collected]\n",
    "\n",
    "    for match in re.finditer(PATTERN_LINK_TEXT, source_page[first_broad_span:last_broad_span]):\n",
    "        text = match.group() + \" \"\n",
    "        span = match.span()\n",
    "        data = (span, text)\n",
    "        collected.append(data)\n",
    "\n",
    "    collected.sort(key=lambda s: s[0])\n",
    "    text_final = \"\"\n",
    "    \n",
    "    for data in collected:\n",
    "        for text in BeautifulSoup(data[1]).find_all('p'):\n",
    "            text_final += text.get_text()\n",
    "\n",
    "    if not title:\n",
    "        title = [\"NULL\"]\n",
    "\n",
    "    return date, title[0], text_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "34904c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(links_list: list):\n",
    "    contents = []\n",
    "    for url in tqdm(links_list, desc=\"Loading...\"):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        date, title, text = collect_data(str(soup))\n",
    "        if any([date, title, text] == \"NULL\"):\n",
    "               pass\n",
    "        else:\n",
    "            contents.append({\"date\": date, \"title\": title, \"article\": text})\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f7ecb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#links = get_links(\"mord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5486446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_content([links[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96e19b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook collect_data_from_links.ipynb to script\n",
      "[NbConvertApp] Writing 3349 bytes to collect_data_from_links.py\n"
     ]
    }
   ],
   "source": [
    "# Export as script\n",
    "!jupyter nbconvert --to script collect_data_from_links.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinos",
   "language": "python",
   "name": "sinos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
